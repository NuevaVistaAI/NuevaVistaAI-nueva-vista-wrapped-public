[
  {
    "id": "ea8a6b82-627f-4303-8289-cd455755fddf",
    "title": "Python Data Analysis Help",
    "create_time": "2024-01-15T00:00:00",
    "update_time": "2024-01-15T00:45:00",
    "mapping": {
      "msg1": {
        "id": "msg1",
        "message": {
          "id": "msg1",
          "author": {
            "role": "user"
          },
          "create_time": "2024-01-15T00:00:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I need help analyzing a dataset with pandas. I have sales data from the last 6 months and want to identify trends, seasonal patterns, and create visualizations. The data includes columns for date, product_category, sales_amount, region, and customer_id. Can you help me create a comprehensive analysis script?"
            ]
          }
        }
      },
      "msg2": {
        "id": "msg2",
        "message": {
          "id": "msg2",
          "author": {
            "role": "assistant"
          },
          "create_time": "2024-01-15T00:02:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I'd be happy to help you create a comprehensive sales data analysis script! Let me break this down into several key components:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load and prepare the data\ndef load_and_clean_data(file_path):\n    \"\"\"\n    Load sales data and perform initial cleaning\n    \"\"\"\n    df = pd.read_csv(file_path)\n    \n    # Convert date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort by date\n    df = df.sort_values('date')\n    \n    # Add time-based features\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['quarter'] = df['date'].dt.quarter\n    df['weekday'] = df['date'].dt.day_name()\n    \n    return df\n\n# Trend Analysis\ndef analyze_trends(df):\n    \"\"\"\n    Analyze sales trends over time\n    \"\"\"\n    # Monthly sales trend\n    monthly_sales = df.groupby(['year', 'month'])['sales_amount'].sum().reset_index()\n    monthly_sales['date'] = pd.to_datetime(monthly_sales[['year', 'month']].assign(day=1))\n    \n    # Calculate growth rates\n    monthly_sales['growth_rate'] = monthly_sales['sales_amount'].pct_change() * 100\n    \n    return monthly_sales\n\n# Seasonal Pattern Analysis\ndef analyze_seasonality(df):\n    \"\"\"\n    Identify seasonal patterns in sales data\n    \"\"\"\n    # Quarterly analysis\n    quarterly_sales = df.groupby('quarter')['sales_amount'].agg(['sum', 'mean', 'count']).reset_index()\n    \n    # Monthly analysis\n    monthly_pattern = df.groupby('month')['sales_amount'].agg(['sum', 'mean']).reset_index()\n    \n    # Weekday analysis\n    weekday_sales = df.groupby('weekday')['sales_amount'].agg(['sum', 'mean']).reset_index()\n    \n    return quarterly_sales, monthly_pattern, weekday_sales\n\n# Regional Analysis\ndef analyze_by_region(df):\n    \"\"\"\n    Analyze sales performance by region\n    \"\"\"\n    regional_performance = df.groupby('region').agg({\n        'sales_amount': ['sum', 'mean', 'count'],\n        'customer_id': 'nunique'\n    }).round(2)\n    \n    regional_performance.columns = ['total_sales', 'avg_sale', 'transaction_count', 'unique_customers']\n    regional_performance = regional_performance.reset_index()\n    \n    return regional_performance\n\n# Product Category Analysis\ndef analyze_by_category(df):\n    \"\"\"\n    Analyze sales by product category\n    \"\"\"\n    category_performance = df.groupby('product_category').agg({\n        'sales_amount': ['sum', 'mean', 'count'],\n        'customer_id': 'nunique'\n    }).round(2)\n    \n    category_performance.columns = ['total_sales', 'avg_sale', 'transaction_count', 'unique_customers']\n    category_performance = category_performance.reset_index()\n    \n    return category_performance\n\n# Visualization Functions\ndef create_visualizations(df, monthly_sales, quarterly_sales, regional_performance, category_performance):\n    \"\"\"\n    Create comprehensive visualizations\n    \"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    fig.suptitle('Sales Data Analysis Dashboard', fontsize=16, fontweight='bold')\n    \n    # 1. Monthly Sales Trend\n    axes[0, 0].plot(monthly_sales['date'], monthly_sales['sales_amount'], marker='o', linewidth=2)\n    axes[0, 0].set_title('Monthly Sales Trend')\n    axes[0, 0].set_xlabel('Date')\n    axes[0, 0].set_ylabel('Sales Amount')\n    axes[0, 0].tick_params(axis='x', rotation=45)\n    \n    # 2. Quarterly Sales Distribution\n    axes[0, 1].bar(quarterly_sales['quarter'], quarterly_sales['sum'])\n    axes[0, 1].set_title('Quarterly Sales Distribution')\n    axes[0, 1].set_xlabel('Quarter')\n    axes[0, 1].set_ylabel('Total Sales')\n    \n    # 3. Regional Performance\n    axes[0, 2].pie(regional_performance['total_sales'], labels=regional_performance['region'], autopct='%1.1f%%')\n    axes[0, 2].set_title('Sales by Region')\n    \n    # 4. Category Performance\n    axes[1, 0].bar(category_performance['product_category'], category_performance['total_sales'])\n    axes[1, 0].set_title('Sales by Product Category')\n    axes[1, 0].set_xlabel('Product Category')\n    axes[1, 0].set_ylabel('Total Sales')\n    axes[1, 0].tick_params(axis='x', rotation=45)\n    \n    # 5. Monthly Growth Rate\n    axes[1, 1].plot(monthly_sales['date'], monthly_sales['growth_rate'], marker='o', color='red')\n    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    axes[1, 1].set_title('Monthly Growth Rate (%)')\n    axes[1, 1].set_xlabel('Date')\n    axes[1, 1].set_ylabel('Growth Rate (%)')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    \n    # 6. Sales Distribution\n    axes[1, 2].hist(df['sales_amount'], bins=30, alpha=0.7, edgecolor='black')\n    axes[1, 2].set_title('Sales Amount Distribution')\n    axes[1, 2].set_xlabel('Sales Amount')\n    axes[1, 2].set_ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main Analysis Function\ndef main_analysis(file_path):\n    \"\"\"\n    Run complete sales data analysis\n    \"\"\"\n    print(\"Loading and cleaning data...\")\n    df = load_and_clean_data(file_path)\n    \n    print(f\"Dataset shape: {df.shape}\")\n    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n    print(f\"Total sales: ${df['sales_amount'].sum():,.2f}\")\n    \n    print(\"\\nAnalyzing trends...\")\n    monthly_sales = analyze_trends(df)\n    \n    print(\"\\nAnalyzing seasonality...\")\n    quarterly_sales, monthly_pattern, weekday_sales = analyze_seasonality(df)\n    \n    print(\"\\nAnalyzing regional performance...\")\n    regional_performance = analyze_by_region(df)\n    \n    print(\"\\nAnalyzing category performance...\")\n    category_performance = analyze_by_category(df)\n    \n    print(\"\\nCreating visualizations...\")\n    create_visualizations(df, monthly_sales, quarterly_sales, regional_performance, category_performance)\n    \n    # Print key insights\n    print(\"\\n\" + \"=\"*50)\n    print(\"KEY INSIGHTS\")\n    print(\"=\"*50)\n    \n    print(f\"\\n\ud83d\udcc8 GROWTH ANALYSIS:\")\n    avg_growth = monthly_sales['growth_rate'].mean()\n    print(f\"Average monthly growth rate: {avg_growth:.1f}%\")\n    \n    print(f\"\\n\ud83c\udfc6 TOP PERFORMERS:\")\n    top_region = regional_performance.loc[regional_performance['total_sales'].idxmax(), 'region']\n    top_category = category_performance.loc[category_performance['total_sales'].idxmax(), 'product_category']\n    print(f\"Top region: {top_region}\")\n    print(f\"Top category: {top_category}\")\n    \n    print(f\"\\n\ud83d\udcca SEASONAL PATTERNS:\")\n    best_quarter = quarterly_sales.loc[quarterly_sales['sum'].idxmax(), 'quarter']\n    print(f\"Best performing quarter: Q{best_quarter}\")\n    \n    return df, monthly_sales, quarterly_sales, regional_performance, category_performance\n\n# Usage example:\n# df, monthly_sales, quarterly_sales, regional_performance, category_performance = main_analysis('sales_data.csv')\n```\n\nThis script provides:\n\n1. **Data Loading & Cleaning**: Handles date conversion and adds time-based features\n2. **Trend Analysis**: Calculates monthly sales trends and growth rates\n3. **Seasonal Analysis**: Identifies patterns by quarter, month, and weekday\n4. **Regional Analysis**: Compares performance across regions\n5. **Category Analysis**: Analyzes product category performance\n6. **Comprehensive Visualizations**: Creates a 6-panel dashboard\n7. **Key Insights**: Automatically identifies top performers and growth patterns\n\nTo use this script:\n\n1. Save it as `sales_analysis.py`\n2. Make sure your CSV has the required columns: date, product_category, sales_amount, region, customer_id\n3. Run: `python sales_analysis.py` or call `main_analysis('your_file.csv')`\n\nWould you like me to modify any part of this analysis or add additional features like customer segmentation, forecasting, or correlation analysis?"
            ]
          }
        }
      },
      "msg3": {
        "id": "msg3",
        "message": {
          "id": "msg3",
          "author": {
            "role": "user"
          },
          "create_time": "2024-01-15T00:08:00",
          "content": {
            "content_type": "text",
            "parts": [
              "This is excellent! Can you also add customer segmentation analysis and some predictive forecasting? I'd like to identify high-value customers and predict next month's sales."
            ]
          }
        }
      },
      "msg4": {
        "id": "msg4",
        "message": {
          "id": "msg4",
          "author": {
            "role": "assistant"
          },
          "create_time": "2024-01-15T00:15:00",
          "content": {
            "content_type": "text",
            "parts": [
              "Absolutely! Let me add customer segmentation and forecasting capabilities to the analysis. Here are the additional functions:\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# Customer Segmentation Analysis\ndef customer_segmentation(df):\n    \"\"\"\n    Perform RFM (Recency, Frequency, Monetary) analysis for customer segmentation\n    \"\"\"\n    # Calculate reference date (latest date in dataset)\n    reference_date = df['date'].max()\n    \n    # Calculate RFM metrics for each customer\n    rfm_data = df.groupby('customer_id').agg({\n        'date': lambda x: (reference_date - x.max()).days,  # Recency\n        'sales_amount': ['count', 'sum']                     # Frequency, Monetary\n    }).reset_index()\n    \n    # Flatten column names\n    rfm_data.columns = ['customer_id', 'recency', 'frequency', 'monetary']\n    \n    # Create RFM scores (1-5 scale)\n    rfm_data['R_score'] = pd.qcut(rfm_data['recency'].rank(method='first'), 5, labels=[5,4,3,2,1])\n    rfm_data['F_score'] = pd.qcut(rfm_data['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n    rfm_data['M_score'] = pd.qcut(rfm_data['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n    \n    # Combine RFM scores\n    rfm_data['RFM_score'] = rfm_data['R_score'].astype(str) + rfm_data['F_score'].astype(str) + rfm_data['M_score'].astype(str)\n    \n    # Customer segmentation based on RFM scores\n    def segment_customers(row):\n        if row['RFM_score'] in ['555', '554', '544', '545', '454', '455', '445']:\n            return 'Champions'\n        elif row['RFM_score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:\n            return 'Loyal Customers'\n        elif row['RFM_score'] in ['512', '511', '422', '421', '412', '411', '311']:\n            return 'Potential Loyalists'\n        elif row['RFM_score'] in ['531', '451', '441', '351']:\n            return 'New Customers'\n        elif row['RFM_score'] in ['155', '154', '144', '214', '215', '115', '114']:\n            return 'At Risk'\n        elif row['RFM_score'] in ['255', '254', '245', '244', '253', '252', '243', '242', '235', '234', '225', '224', '153', '152', '145', '143', '142', '135', '134', '125', '124']:\n            return 'Cannot Lose Them'\n        elif row['RFM_score'] in ['155', '154', '144', '214', '215', '115', '114']:\n            return 'Hibernating'\n        else:\n            return 'Lost'\n    \n    rfm_data['segment'] = rfm_data.apply(segment_customers, axis=1)\n    \n    # Customer value analysis\n    rfm_data['customer_value'] = rfm_data['monetary'] / rfm_data['frequency']  # Average order value\n    \n    return rfm_data\n\n# Advanced Customer Clustering\ndef advanced_customer_clustering(df, rfm_data, n_clusters=5):\n    \"\"\"\n    Perform K-means clustering on customer data\n    \"\"\"\n    # Prepare features for clustering\n    features = ['recency', 'frequency', 'monetary']\n    X = rfm_data[features].copy()\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    rfm_data['cluster'] = kmeans.fit_predict(X_scaled)\n    \n    # Analyze clusters\n    cluster_analysis = rfm_data.groupby('cluster').agg({\n        'recency': 'mean',\n        'frequency': 'mean', \n        'monetary': 'mean',\n        'customer_value': 'mean',\n        'customer_id': 'count'\n    }).round(2)\n    cluster_analysis.columns = ['avg_recency', 'avg_frequency', 'avg_monetary', 'avg_order_value', 'customer_count']\n    \n    return rfm_data, cluster_analysis\n\n# Sales Forecasting\ndef sales_forecasting(df, forecast_months=3):\n    \"\"\"\n    Predict future sales using multiple models\n    \"\"\"\n    # Prepare monthly sales data\n    monthly_data = df.groupby(['year', 'month'])['sales_amount'].sum().reset_index()\n    monthly_data['date'] = pd.to_datetime(monthly_data[['year', 'month']].assign(day=1))\n    monthly_data = monthly_data.sort_values('date')\n    \n    # Create features for modeling\n    monthly_data['month_num'] = range(len(monthly_data))\n    monthly_data['sales_lag1'] = monthly_data['sales_amount'].shift(1)\n    monthly_data['sales_lag2'] = monthly_data['sales_amount'].shift(2)\n    monthly_data['sales_lag3'] = monthly_data['sales_amount'].shift(3)\n    monthly_data['rolling_mean_3'] = monthly_data['sales_amount'].rolling(window=3).mean()\n    \n    # Remove rows with NaN values\n    model_data = monthly_data.dropna()\n    \n    # Prepare features and target\n    feature_cols = ['month_num', 'month', 'sales_lag1', 'sales_lag2', 'sales_lag3', 'rolling_mean_3']\n    X = model_data[feature_cols]\n    y = model_data['sales_amount']\n    \n    # Split data (use last 20% for testing)\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n    \n    # Train models\n    models = {\n        'Linear Regression': LinearRegression(),\n        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n    }\n    \n    model_performance = {}\n    trained_models = {}\n    \n    for name, model in models.items():\n        # Train model\n        model.fit(X_train, y_train)\n        trained_models[name] = model\n        \n        # Make predictions on test set\n        y_pred = model.predict(X_test)\n        \n        # Calculate metrics\n        mae = mean_absolute_error(y_test, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n        \n        model_performance[name] = {\n            'MAE': mae,\n            'RMSE': rmse,\n            'MAPE': mape\n        }\n    \n    # Choose best model (lowest MAPE)\n    best_model_name = min(model_performance.keys(), key=lambda x: model_performance[x]['MAPE'])\n    best_model = trained_models[best_model_name]\n    \n    # Generate future predictions\n    last_row = model_data.iloc[-1]\n    future_predictions = []\n    \n    for i in range(forecast_months):\n        # Create features for next month\n        next_month_features = {\n            'month_num': last_row['month_num'] + i + 1,\n            'month': ((last_row['month'] + i) % 12) + 1,\n            'sales_lag1': last_row['sales_amount'] if i == 0 else future_predictions[i-1],\n            'sales_lag2': last_row['sales_lag1'] if i == 0 else (last_row['sales_amount'] if i == 1 else future_predictions[i-2]),\n            'sales_lag3': last_row['sales_lag2'] if i == 0 else (last_row['sales_lag1'] if i == 1 else (last_row['sales_amount'] if i == 2 else future_predictions[i-3])),\n            'rolling_mean_3': np.mean([last_row['sales_amount'], last_row['sales_lag1'], last_row['sales_lag2']]) if i == 0 else np.mean(future_predictions[max(0, i-2):i+1] + [last_row['sales_amount']])\n        }\n        \n        # Make prediction\n        X_next = pd.DataFrame([next_month_features])\n        pred = best_model.predict(X_next)[0]\n        future_predictions.append(pred)\n    \n    return model_performance, future_predictions, best_model_name\n\n# Enhanced Visualization Function\ndef create_enhanced_visualizations(df, rfm_data, cluster_analysis, model_performance, future_predictions):\n    \"\"\"\n    Create enhanced visualizations including segmentation and forecasting\n    \"\"\"\n    fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n    fig.suptitle('Enhanced Sales Analytics Dashboard', fontsize=18, fontweight='bold')\n    \n    # Customer Segmentation Distribution\n    segment_counts = rfm_data['segment'].value_counts()\n    axes[0, 0].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')\n    axes[0, 0].set_title('Customer Segmentation Distribution')\n    \n    # RFM Analysis Scatter Plot\n    scatter = axes[0, 1].scatter(rfm_data['frequency'], rfm_data['monetary'], \n                                c=rfm_data['recency'], cmap='viridis', alpha=0.6)\n    axes[0, 1].set_xlabel('Frequency')\n    axes[0, 1].set_ylabel('Monetary')\n    axes[0, 1].set_title('Customer RFM Analysis')\n    plt.colorbar(scatter, ax=axes[0, 1], label='Recency (days)')\n    \n    # Customer Value Distribution\n    axes[0, 2].hist(rfm_data['customer_value'], bins=30, alpha=0.7, edgecolor='black')\n    axes[0, 2].set_xlabel('Customer Value ($)')\n    axes[0, 2].set_ylabel('Frequency')\n    axes[0, 2].set_title('Customer Value Distribution')\n    \n    # Cluster Analysis\n    cluster_summary = cluster_analysis[['avg_frequency', 'avg_monetary', 'customer_count']]\n    cluster_summary.plot(kind='bar', ax=axes[1, 0])\n    axes[1, 0].set_title('Customer Clusters Analysis')\n    axes[1, 0].set_xlabel('Cluster')\n    axes[1, 0].tick_params(axis='x', rotation=0)\n    \n    # Sales Forecast Visualization\n    monthly_sales = df.groupby(['year', 'month'])['sales_amount'].sum().reset_index()\n    monthly_sales['date'] = pd.to_datetime(monthly_sales[['year', 'month']].assign(day=1))\n    \n    # Plot historical data\n    axes[1, 1].plot(monthly_sales['date'], monthly_sales['sales_amount'], \n                   marker='o', label='Historical', linewidth=2)\n    \n    # Plot forecasted data\n    last_date = monthly_sales['date'].max()\n    future_dates = [last_date + pd.DateOffset(months=i+1) for i in range(len(future_predictions))]\n    axes[1, 1].plot(future_dates, future_predictions, \n                   marker='s', label='Forecast', linewidth=2, linestyle='--', color='red')\n    \n    axes[1, 1].set_title('Sales Forecast')\n    axes[1, 1].set_xlabel('Date')\n    axes[1, 1].set_ylabel('Sales Amount')\n    axes[1, 1].legend()\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    \n    # Model Performance Comparison\n    model_names = list(model_performance.keys())\n    mape_scores = [model_performance[model]['MAPE'] for model in model_names]\n    axes[1, 2].bar(model_names, mape_scores)\n    axes[1, 2].set_title('Model Performance (MAPE)')\n    axes[1, 2].set_ylabel('MAPE (%)')\n    \n    # Top Customer Analysis\n    top_customers = rfm_data.nlargest(10, 'monetary')\n    axes[2, 0].barh(range(len(top_customers)), top_customers['monetary'])\n    axes[2, 0].set_yticks(range(len(top_customers)))\n    axes[2, 0].set_yticklabels([f'Customer {i+1}' for i in range(len(top_customers))])\n    axes[2, 0].set_xlabel('Total Sales ($)')\n    axes[2, 0].set_title('Top 10 Customers by Revenue')\n    \n    # Customer Lifetime Value vs Recency\n    axes[2, 1].scatter(rfm_data['recency'], rfm_data['monetary'], alpha=0.6)\n    axes[2, 1].set_xlabel('Recency (days since last purchase)')\n    axes[2, 1].set_ylabel('Customer Lifetime Value ($)')\n    axes[2, 1].set_title('Customer Value vs Recency')\n    \n    # Monthly Sales Growth Rate\n    monthly_sales['growth_rate'] = monthly_sales['sales_amount'].pct_change() * 100\n    axes[2, 2].plot(monthly_sales['date'], monthly_sales['growth_rate'], marker='o')\n    axes[2, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n    axes[2, 2].set_xlabel('Date')\n    axes[2, 2].set_ylabel('Growth Rate (%)')\n    axes[2, 2].set_title('Monthly Sales Growth Rate')\n    axes[2, 2].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Updated Main Analysis Function\ndef enhanced_main_analysis(file_path):\n    \"\"\"\n    Run enhanced sales data analysis with segmentation and forecasting\n    \"\"\"\n    print(\"Loading and cleaning data...\")\n    df = load_and_clean_data(file_path)\n    \n    print(\"\\nPerforming customer segmentation...\")\n    rfm_data = customer_segmentation(df)\n    \n    print(\"\\nPerforming advanced clustering...\")\n    rfm_data, cluster_analysis = advanced_customer_clustering(df, rfm_data)\n    \n    print(\"\\nGenerating sales forecasts...\")\n    model_performance, future_predictions, best_model = sales_forecasting(df)\n    \n    print(\"\\nCreating enhanced visualizations...\")\n    create_enhanced_visualizations(df, rfm_data, cluster_analysis, model_performance, future_predictions)\n    \n    # Enhanced insights\n    print(\"\\n\" + \"=\"*60)\n    print(\"ENHANCED INSIGHTS\")\n    print(\"=\"*60)\n    \n    print(f\"\\n\ud83c\udfaf CUSTOMER SEGMENTATION:\")\n    segment_summary = rfm_data['segment'].value_counts()\n    for segment, count in segment_summary.head(3).items():\n        percentage = (count / len(rfm_data)) * 100\n        print(f\"{segment}: {count} customers ({percentage:.1f}%)\")\n    \n    print(f\"\\n\ud83d\udc8e HIGH-VALUE CUSTOMERS:\")\n    champions = rfm_data[rfm_data['segment'] == 'Champions']\n    if len(champions) > 0:\n        avg_value = champions['monetary'].mean()\n        print(f\"Champions segment: ${avg_value:,.2f} average lifetime value\")\n    \n    top_customer_value = rfm_data['monetary'].max()\n    print(f\"Top customer lifetime value: ${top_customer_value:,.2f}\")\n    \n    print(f\"\\n\ud83d\udcc8 SALES FORECAST:\")\n    print(f\"Best forecasting model: {best_model}\")\n    total_forecast = sum(future_predictions)\n    print(f\"Next 3 months predicted sales: ${total_forecast:,.2f}\")\n    \n    print(f\"\\n\u26a0\ufe0f  AT-RISK CUSTOMERS:\")\n    at_risk = rfm_data[rfm_data['segment'].isin(['At Risk', 'Cannot Lose Them'])]\n    if len(at_risk) > 0:\n        at_risk_value = at_risk['monetary'].sum()\n        print(f\"{len(at_risk)} customers at risk (${at_risk_value:,.2f} total value)\")\n    \n    return df, rfm_data, cluster_analysis, model_performance, future_predictions\n\n# Usage:\n# df, rfm_data, cluster_analysis, model_performance, future_predictions = enhanced_main_analysis('sales_data.csv')\n```\n\nThis enhanced version adds:\n\n### Customer Segmentation:\n- **RFM Analysis**: Segments customers based on Recency, Frequency, and Monetary value\n- **Customer Segments**: Champions, Loyal Customers, At Risk, etc.\n- **K-means Clustering**: Advanced clustering for deeper customer insights\n- **Customer Lifetime Value**: Identifies high-value customers\n\n### Predictive Forecasting:\n- **Multiple Models**: Linear Regression and Random Forest\n- **Model Evaluation**: MAE, RMSE, and MAPE metrics\n- **Feature Engineering**: Uses lag variables and rolling averages\n- **Future Predictions**: 3-month sales forecast\n\n### Enhanced Visualizations:\n- Customer segmentation pie chart\n- RFM scatter plot with recency coloring\n- Customer value distribution\n- Sales forecast line chart\n- Model performance comparison\n- Top customers analysis\n\nThis gives you a complete customer analytics and forecasting solution! Would you like me to add any other features like churn prediction or marketing ROI analysis?"
            ]
          }
        }
      }
    }
  },
  {
    "id": "4eb0085d-10ae-44db-876e-ac8a272ba2ee",
    "title": "Machine Learning Project Setup",
    "create_time": "2024-01-20T00:00:00",
    "update_time": "2024-01-20T00:25:00",
    "mapping": {
      "msg1": {
        "id": "msg1",
        "message": {
          "id": "msg1",
          "author": {
            "role": "user"
          },
          "create_time": "2024-01-20T00:00:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I'm starting a new machine learning project to predict customer churn for a subscription service. Can you help me set up the project structure and create the initial data preprocessing pipeline?"
            ]
          }
        }
      },
      "msg2": {
        "id": "msg2",
        "message": {
          "id": "msg2",
          "author": {
            "role": "assistant"
          },
          "create_time": "2024-01-20T00:03:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I'll help you set up a comprehensive machine learning project for customer churn prediction! Let me create a professional project structure and preprocessing pipeline.\n\n## Project Structure\n\nFirst, let's create the optimal directory structure:\n\n```\nchurn-prediction/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502   \u251c\u2500\u2500 02_feature_engineering.ipynb\n\u2502   \u2514\u2500\u2500 03_model_development.ipynb\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data_loader.py\n\u2502   \u2502   \u2514\u2500\u2500 preprocessing.py\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 feature_engineering.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 train_model.py\n\u2502   \u2502   \u2514\u2500\u2500 predict_model.py\n\u2502   \u2514\u2500\u2500 visualization/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 visualize.py\n\u251c\u2500\u2500 models/\n\u251c\u2500\u2500 reports/\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 config.yaml\n```\n\n## Core Data Preprocessing Pipeline\n\n```python\n# src/data/preprocessing.py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport logging\nimport yaml\n\nclass ChurnDataPreprocessor:\n    \"\"\"\n    Comprehensive data preprocessing pipeline for churn prediction\n    \"\"\"\n    \n    def __init__(self, config_path='config.yaml'):\n        with open(config_path, 'r') as file:\n            self.config = yaml.safe_load(file)\n        \n        self.scaler = StandardScaler()\n        self.label_encoders = {}\n        self.imputers = {}\n        \n        # Set up logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n    \n    def load_data(self, file_path):\n        \"\"\"\n        Load raw data from CSV file\n        \"\"\"\n        try:\n            df = pd.read_csv(file_path)\n            self.logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n            return df\n        except Exception as e:\n            self.logger.error(f\"Error loading data: {e}\")\n            raise\n    \n    def explore_data(self, df):\n        \"\"\"\n        Generate basic data exploration report\n        \"\"\"\n        exploration_report = {\n            'shape': df.shape,\n            'missing_values': df.isnull().sum().to_dict(),\n            'data_types': df.dtypes.to_dict(),\n            'numeric_summary': df.describe().to_dict(),\n            'categorical_unique': {col: df[col].nunique() \n                                 for col in df.select_dtypes(include=['object']).columns}\n        }\n        \n        return exploration_report\n    \n    def handle_missing_values(self, df, strategy='mean'):\n        \"\"\"\n        Handle missing values in the dataset\n        \"\"\"\n        df_processed = df.copy()\n        \n        # Separate numeric and categorical columns\n        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n        categorical_cols = df_processed.select_dtypes(include=['object']).columns\n        \n        # Handle numeric missing values\n        if len(numeric_cols) > 0:\n            numeric_imputer = SimpleImputer(strategy=strategy)\n            df_processed[numeric_cols] = numeric_imputer.fit_transform(df_processed[numeric_cols])\n            self.imputers['numeric'] = numeric_imputer\n        \n        # Handle categorical missing values\n        if len(categorical_cols) > 0:\n            categorical_imputer = SimpleImputer(strategy='most_frequent')\n            df_processed[categorical_cols] = categorical_imputer.fit_transform(df_processed[categorical_cols])\n            self.imputers['categorical'] = categorical_imputer\n        \n        self.logger.info(f\"Missing values handled. Strategy: {strategy}\")\n        return df_processed\n    \n    def create_features(self, df):\n        \"\"\"\n        Create new features based on existing ones\n        \"\"\"\n        df_featured = df.copy()\n        \n        # Example feature engineering for subscription churn\n        if 'tenure_months' in df_featured.columns and 'monthly_charges' in df_featured.columns:\n            df_featured['total_charges_calculated'] = df_featured['tenure_months'] * df_featured['monthly_charges']\n            df_featured['charges_per_month_ratio'] = df_featured['monthly_charges'] / (df_featured['tenure_months'] + 1)\n        \n        if 'total_charges' in df_featured.columns and 'monthly_charges' in df_featured.columns:\n            df_featured['avg_monthly_spend'] = df_featured['total_charges'] / (df_featured['tenure_months'] + 1)\n            df_featured['charge_increase_rate'] = (df_featured['monthly_charges'] - df_featured['avg_monthly_spend']) / df_featured['avg_monthly_spend']\n        \n        # Create tenure categories\n        if 'tenure_months' in df_featured.columns:\n            def categorize_tenure(tenure):\n                if tenure <= 12:\n                    return 'New'\n                elif tenure <= 36:\n                    return 'Medium'\n                else:\n                    return 'Long'\n            \n            df_featured['tenure_category'] = df_featured['tenure_months'].apply(categorize_tenure)\n        \n        # Create service usage features\n        service_cols = [col for col in df_featured.columns if col.startswith('service_') or col.endswith('_service')]\n        if service_cols:\n            df_featured['total_services'] = df_featured[service_cols].sum(axis=1)\n        \n        self.logger.info(f\"Feature engineering completed. New shape: {df_featured.shape}\")\n        return df_featured\n    \n    def encode_categorical_variables(self, df, target_column=None):\n        \"\"\"\n        Encode categorical variables\n        \"\"\"\n        df_encoded = df.copy()\n        categorical_cols = df_encoded.select_dtypes(include=['object']).columns\n        \n        if target_column:\n            categorical_cols = categorical_cols.drop(target_column)\n        \n        for col in categorical_cols:\n            if df_encoded[col].nunique() <= 2:\n                # Binary encoding for binary categorical variables\n                le = LabelEncoder()\n                df_encoded[col] = le.fit_transform(df_encoded[col])\n                self.label_encoders[col] = le\n            else:\n                # One-hot encoding for multi-category variables\n                df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col)\n        \n        self.logger.info(f\"Categorical encoding completed. New shape: {df_encoded.shape}\")\n        return df_encoded\n    \n    def scale_features(self, df, target_column=None):\n        \"\"\"\n        Scale numerical features\n        \"\"\"\n        df_scaled = df.copy()\n        \n        # Get numeric columns (excluding target)\n        numeric_cols = df_scaled.select_dtypes(include=[np.number]).columns\n        if target_column and target_column in numeric_cols:\n            numeric_cols = numeric_cols.drop(target_column)\n        \n        if len(numeric_cols) > 0:\n            df_scaled[numeric_cols] = self.scaler.fit_transform(df_scaled[numeric_cols])\n            self.logger.info(f\"Feature scaling completed for {len(numeric_cols)} columns\")\n        \n        return df_scaled\n    \n    def split_data(self, df, target_column, test_size=0.2, random_state=42):\n        \"\"\"\n        Split data into training and testing sets\n        \"\"\"\n        X = df.drop(columns=[target_column])\n        y = df[target_column]\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=random_state, stratify=y\n        )\n        \n        self.logger.info(f\"Data split completed. Train: {X_train.shape}, Test: {X_test.shape}\")\n        return X_train, X_test, y_train, y_test\n    \n    def full_pipeline(self, file_path, target_column='churn'):\n        \"\"\"\n        Execute the complete preprocessing pipeline\n        \"\"\"\n        self.logger.info(\"Starting full preprocessing pipeline...\")\n        \n        # Load data\n        df = self.load_data(file_path)\n        \n        # Data exploration\n        exploration_report = self.explore_data(df)\n        \n        # Handle missing values\n        df = self.handle_missing_values(df)\n        \n        # Feature engineering\n        df = self.create_features(df)\n        \n        # Encode target variable if it's categorical\n        if target_column in df.select_dtypes(include=['object']).columns:\n            le_target = LabelEncoder()\n            df[target_column] = le_target.fit_transform(df[target_column])\n            self.label_encoders[target_column] = le_target\n        \n        # Encode categorical variables\n        df = self.encode_categorical_variables(df, target_column)\n        \n        # Scale features\n        df = self.scale_features(df, target_column)\n        \n        # Split data\n        X_train, X_test, y_train, y_test = self.split_data(df, target_column)\n        \n        self.logger.info(\"Preprocessing pipeline completed successfully!\")\n        \n        return {\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'exploration_report': exploration_report,\n            'feature_names': X_train.columns.tolist()\n        }\n    \n    def save_processed_data(self, data_dict, output_dir='data/processed/'):\n        \"\"\"\n        Save processed data to files\n        \"\"\"\n        import os\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Save datasets\n        data_dict['X_train'].to_csv(f'{output_dir}/X_train.csv', index=False)\n        data_dict['X_test'].to_csv(f'{output_dir}/X_test.csv', index=False)\n        data_dict['y_train'].to_csv(f'{output_dir}/y_train.csv', index=False)\n        data_dict['y_test'].to_csv(f'{output_dir}/y_test.csv', index=False)\n        \n        # Save exploration report\n        import json\n        with open(f'{output_dir}/exploration_report.json', 'w') as f:\n            json.dump(data_dict['exploration_report'], f, indent=2, default=str)\n        \n        self.logger.info(f\"Processed data saved to {output_dir}\")\n\n# Usage example:\n# preprocessor = ChurnDataPreprocessor()\n# processed_data = preprocessor.full_pipeline('data/raw/customer_data.csv')\n# preprocessor.save_processed_data(processed_data)\n```\n\n## Configuration File (config.yaml)\n\n```yaml\n# config.yaml\ndata:\n  raw_data_path: 'data/raw/customer_data.csv'\n  processed_data_path: 'data/processed/'\n  target_column: 'churn'\n\npreprocessing:\n  missing_value_strategy: 'mean'\n  test_size: 0.2\n  random_state: 42\n  scaling_method: 'standard'\n\nfeature_engineering:\n  create_tenure_categories: true\n  create_service_counts: true\n  create_ratio_features: true\n\nmodel:\n  algorithms: ['logistic_regression', 'random_forest', 'xgboost']\n  cross_validation_folds: 5\n  scoring_metric: 'roc_auc'\n\nlogging:\n  level: 'INFO'\n  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n```\n\n## Requirements File\n\n```txt\n# requirements.txt\npandas>=1.3.3\nnumpy>=1.21.2\nscikit-learn>=1.0.0\nxgboost>=1.5.0\nmatplotlib>=3.4.3\nseaborn>=0.11.2\njupyter>=1.0.0\nPyYAML>=5.4.1\njoblib>=1.1.0\nplotly>=5.3.0\nshap>=0.40.0\n```\n\n## Quick Start Script\n\n```python\n# main.py\nfrom src.data.preprocessing import ChurnDataPreprocessor\nimport logging\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    \n    # Set up logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    \n    # Initialize preprocessor\n    preprocessor = ChurnDataPreprocessor('config.yaml')\n    \n    # Run full pipeline\n    processed_data = preprocessor.full_pipeline('data/raw/customer_data.csv')\n    \n    # Save processed data\n    preprocessor.save_processed_data(processed_data)\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*50)\n    print(\"PREPROCESSING SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Training samples: {len(processed_data['X_train'])}\")\n    print(f\"Test samples: {len(processed_data['X_test'])}\")\n    print(f\"Features: {len(processed_data['feature_names'])}\")\n    print(f\"Target distribution:\")\n    print(processed_data['y_train'].value_counts())\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nThis setup provides:\n\n1. **Professional Structure**: Organized directories for different components\n2. **Comprehensive Preprocessing**: Handles missing values, feature engineering, encoding, and scaling\n3. **Configurable Pipeline**: Uses YAML config for easy parameter adjustment\n4. **Logging**: Tracks all preprocessing steps\n5. **Modular Design**: Easy to extend and modify\n6. **Data Validation**: Built-in exploration and validation\n\nWould you like me to add model training components or create specific visualization functions for churn analysis?"
            ]
          }
        }
      },
      "msg3": {
        "id": "msg3",
        "message": {
          "id": "msg3",
          "author": {
            "role": "user"
          },
          "create_time": "2024-01-20T00:12:00",
          "content": {
            "content_type": "text",
            "parts": [
              "This is great! Can you also add the model training pipeline with hyperparameter tuning and evaluation metrics?"
            ]
          }
        }
      }
    }
  },
  {
    "id": "944346d9-06db-47a8-94bd-e1e30e8acc1d",
    "title": "Quick Automation Script",
    "create_time": "2024-01-27T00:00:00",
    "update_time": "2024-01-27T00:08:00",
    "mapping": {
      "msg1": {
        "id": "msg1",
        "message": {
          "id": "msg1",
          "author": {
            "role": "user"
          },
          "create_time": "2024-01-27T00:00:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I need a Python script to automatically organize my Downloads folder by file type. Can you create something that sorts files into subfolders?"
            ]
          }
        }
      },
      "msg2": {
        "id": "msg2",
        "message": {
          "id": "msg2",
          "author": {
            "role": "assistant"
          },
          "create_time": "2024-01-27T00:05:00",
          "content": {
            "content_type": "text",
            "parts": [
              "I'll create a comprehensive file organizer script for your Downloads folder! Here's a robust solution:\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nDownloads Folder Organizer\nAutomatically sorts files in Downloads folder by type into organized subfolders.\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport argparse\n\nclass DownloadsOrganizer:\n    \"\"\"\n    Organize downloads folder by file type\n    \"\"\"\n    \n    def __init__(self, downloads_path=None):\n        # Default to user's Downloads folder\n        if downloads_path is None:\n            self.downloads_path = Path.home() / 'Downloads'\n        else:\n            self.downloads_path = Path(downloads_path)\n        \n        # File type categories\n        self.file_categories = {\n            'Images': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico', '.tiff'],\n            'Documents': ['.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt', '.pages'],\n            'Spreadsheets': ['.xls', '.xlsx', '.csv', '.ods', '.numbers'],\n            'Presentations': ['.ppt', '.pptx', '.odp', '.key'],\n            'Videos': ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v'],\n            'Audio': ['.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a'],\n            'Archives': ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2', '.xz'],\n            'Code': ['.py', '.js', '.html', '.css', '.java', '.cpp', '.c', '.php', '.rb', '.go'],\n            'Executables': ['.exe', '.msi', '.deb', '.rpm', '.dmg', '.pkg', '.app'],\n            'Data': ['.json', '.xml', '.yaml', '.yml', '.sql', '.db', '.sqlite']\n        }\n        \n        # Set up logging\n        self.setup_logging()\n    \n    def setup_logging(self):\n        \"\"\"Set up logging configuration\"\"\"\n        log_format = '%(asctime)s - %(levelname)s - %(message)s'\n        logging.basicConfig(\n            level=logging.INFO,\n            format=log_format,\n            handlers=[\n                logging.FileHandler(self.downloads_path / 'organizer.log'),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n    \n    def get_file_category(self, file_path):\n        \"\"\"Determine the category of a file based on its extension\"\"\"\n        file_extension = file_path.suffix.lower()\n        \n        for category, extensions in self.file_categories.items():\n            if file_extension in extensions:\n                return category\n        \n        return 'Others'  # Default category\n    \n    def create_category_folders(self):\n        \"\"\"Create category folders if they don't exist\"\"\"\n        created_folders = []\n        \n        for category in list(self.file_categories.keys()) + ['Others']:\n            folder_path = self.downloads_path / category\n            if not folder_path.exists():\n                folder_path.mkdir(exist_ok=True)\n                created_folders.append(category)\n                self.logger.info(f\"Created folder: {category}\")\n        \n        return created_folders\n    \n    def move_file(self, source_path, destination_folder):\n        \"\"\"Move file to destination folder with conflict resolution\"\"\"\n        destination_path = self.downloads_path / destination_folder / source_path.name\n        \n        # Handle filename conflicts\n        if destination_path.exists():\n            base_name = source_path.stem\n            extension = source_path.suffix\n            counter = 1\n            \n            while destination_path.exists():\n                new_name = f\"{base_name}_{counter}{extension}\"\n                destination_path = self.downloads_path / destination_folder / new_name\n                counter += 1\n        \n        try:\n            shutil.move(str(source_path), str(destination_path))\n            self.logger.info(f\"Moved {source_path.name} to {destination_folder}/\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error moving {source_path.name}: {e}\")\n            return False\n    \n    def organize_files(self, dry_run=False):\n        \"\"\"Organize all files in the downloads folder\"\"\"\n        if not self.downloads_path.exists():\n            self.logger.error(f\"Downloads folder not found: {self.downloads_path}\")\n            return False\n        \n        self.logger.info(f\"Starting organization of: {self.downloads_path}\")\n        \n        # Create category folders\n        if not dry_run:\n            self.create_category_folders()\n        \n        # Get all files (excluding directories and log files)\n        files_to_organize = []\n        for item in self.downloads_path.iterdir():\n            if (item.is_file() and \n                not item.name.startswith('.') and \n                item.name != 'organizer.log' and\n                item.parent.name == 'Downloads'):  # Only files directly in Downloads\n                files_to_organize.append(item)\n        \n        if not files_to_organize:\n            self.logger.info(\"No files to organize.\")\n            return True\n        \n        # Organize files\n        moved_count = 0\n        organization_summary = {}\n        \n        for file_path in files_to_organize:\n            category = self.get_file_category(file_path)\n            \n            if category not in organization_summary:\n                organization_summary[category] = []\n            \n            organization_summary[category].append(file_path.name)\n            \n            if dry_run:\n                self.logger.info(f\"[DRY RUN] Would move {file_path.name} to {category}/\")\n            else:\n                if self.move_file(file_path, category):\n                    moved_count += 1\n        \n        # Print summary\n        self.print_summary(organization_summary, moved_count, dry_run)\n        return True\n    \n    def print_summary(self, organization_summary, moved_count, dry_run):\n        \"\"\"Print organization summary\"\"\"\n        action = \"Would organize\" if dry_run else \"Organized\"\n        \n        print(f\"\\n{'='*50}\")\n        print(f\"DOWNLOADS FOLDER ORGANIZATION SUMMARY\")\n        print(f\"{'='*50}\")\n        print(f\"\ud83d\udcc1 Location: {self.downloads_path}\")\n        print(f\"\ud83d\udcca {action}: {sum(len(files) for files in organization_summary.values())} files\")\n        \n        if not dry_run:\n            print(f\"\u2705 Successfully moved: {moved_count} files\")\n        \n        print(f\"\\n\ud83d\udccb Files by category:\")\n        for category, files in organization_summary.items():\n            print(f\"\\n  {category} ({len(files)} files):\")\n            for file_name in files[:5]:  # Show first 5 files\n                print(f\"    \u2022 {file_name}\")\n            if len(files) > 5:\n                print(f\"    \u2022 ... and {len(files) - 5} more\")\n        \n        if not dry_run:\n            print(f\"\\n\ud83d\udcdd Log file: {self.downloads_path / 'organizer.log'}\")\n    \n    def undo_organization(self):\n        \"\"\"Move all files back to Downloads root (undo organization)\"\"\"\n        print(\"Undoing organization...\")\n        \n        moved_count = 0\n        for category in list(self.file_categories.keys()) + ['Others']:\n            category_folder = self.downloads_path / category\n            \n            if category_folder.exists() and category_folder.is_dir():\n                for file_path in category_folder.iterdir():\n                    if file_path.is_file():\n                        destination = self.downloads_path / file_path.name\n                        \n                        # Handle conflicts\n                        if destination.exists():\n                            base_name = file_path.stem\n                            extension = file_path.suffix\n                            counter = 1\n                            \n                            while destination.exists():\n                                new_name = f\"{base_name}_restored_{counter}{extension}\"\n                                destination = self.downloads_path / new_name\n                                counter += 1\n                        \n                        try:\n                            shutil.move(str(file_path), str(destination))\n                            moved_count += 1\n                            self.logger.info(f\"Restored {file_path.name} to Downloads root\")\n                        except Exception as e:\n                            self.logger.error(f\"Error restoring {file_path.name}: {e}\")\n                \n                # Remove empty category folder\n                try:\n                    if not any(category_folder.iterdir()):\n                        category_folder.rmdir()\n                        self.logger.info(f\"Removed empty folder: {category}\")\n                except:\n                    pass\n        \n        print(f\"\\n\u2705 Restored {moved_count} files to Downloads root\")\n    \n    def get_statistics(self):\n        \"\"\"Get statistics about the Downloads folder\"\"\"\n        stats = {\n            'total_files': 0,\n            'total_size': 0,\n            'categories': {},\n            'largest_files': [],\n            'oldest_files': [],\n            'newest_files': []\n        }\n        \n        all_files = []\n        for item in self.downloads_path.rglob('*'):\n            if item.is_file() and not item.name.startswith('.'):\n                all_files.append(item)\n        \n        for file_path in all_files:\n            # Basic stats\n            stats['total_files'] += 1\n            file_size = file_path.stat().st_size\n            stats['total_size'] += file_size\n            \n            # Category stats\n            category = self.get_file_category(file_path)\n            if category not in stats['categories']:\n                stats['categories'][category] = {'count': 0, 'size': 0}\n            stats['categories'][category]['count'] += 1\n            stats['categories'][category]['size'] += file_size\n            \n            # File info for sorting\n            file_info = {\n                'name': file_path.name,\n                'size': file_size,\n                'modified': file_path.stat().st_mtime\n            }\n            all_files_info.append(file_info)\n        \n        # Sort for largest, oldest, newest\n        all_files_info = [{\n            'name': f.name,\n            'size': f.stat().st_size,\n            'modified': f.stat().st_mtime\n        } for f in all_files]\n        \n        stats['largest_files'] = sorted(all_files_info, key=lambda x: x['size'], reverse=True)[:5]\n        stats['oldest_files'] = sorted(all_files_info, key=lambda x: x['modified'])[:5]\n        stats['newest_files'] = sorted(all_files_info, key=lambda x: x['modified'], reverse=True)[:5]\n        \n        return stats\n\ndef format_bytes(bytes_value):\n    \"\"\"Convert bytes to human readable format\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if bytes_value < 1024.0:\n            return f\"{bytes_value:.1f} {unit}\"\n        bytes_value /= 1024.0\n    return f\"{bytes_value:.1f} TB\"\n\ndef main():\n    parser = argparse.ArgumentParser(description='Organize Downloads folder by file type')\n    parser.add_argument('--path', '-p', type=str, help='Path to downloads folder (default: ~/Downloads)')\n    parser.add_argument('--dry-run', '-d', action='store_true', help='Preview changes without moving files')\n    parser.add_argument('--undo', '-u', action='store_true', help='Undo organization (move files back)')\n    parser.add_argument('--stats', '-s', action='store_true', help='Show folder statistics')\n    \n    args = parser.parse_args()\n    \n    # Initialize organizer\n    organizer = DownloadsOrganizer(args.path)\n    \n    if args.stats:\n        print(\"Calculating folder statistics...\")\n        stats = organizer.get_statistics()\n        \n        print(f\"\\n{'='*50}\")\n        print(f\"DOWNLOADS FOLDER STATISTICS\")\n        print(f\"{'='*50}\")\n        print(f\"\ud83d\udcc1 Location: {organizer.downloads_path}\")\n        print(f\"\ud83d\udcca Total files: {stats['total_files']}\")\n        print(f\"\ud83d\udcbe Total size: {format_bytes(stats['total_size'])}\")\n        \n        print(f\"\\n\ud83d\udccb Files by category:\")\n        for category, info in stats['categories'].items():\n            print(f\"  {category}: {info['count']} files ({format_bytes(info['size'])})\")\n        \n    elif args.undo:\n        organizer.undo_organization()\n    else:\n        organizer.organize_files(dry_run=args.dry_run)\n        \n        if args.dry_run:\n            print(\"\\n\ud83d\udca1 Run without --dry-run to actually move the files.\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Usage Examples:\n\n```bash\n# Preview what would be organized (dry run)\npython downloads_organizer.py --dry-run\n\n# Actually organize the files\npython downloads_organizer.py\n\n# Organize a specific folder\npython downloads_organizer.py --path \"/path/to/folder\"\n\n# Show statistics about your downloads\npython downloads_organizer.py --stats\n\n# Undo organization (move files back)\npython downloads_organizer.py --undo\n```\n\n## Key Features:\n\n\u2705 **Smart Organization**: Sorts files into logical categories\n\u2705 **Conflict Resolution**: Handles duplicate filenames automatically\n\u2705 **Dry Run Mode**: Preview changes before applying\n\u2705 **Undo Function**: Restore original organization\n\u2705 **Detailed Logging**: Track all operations\n\u2705 **Statistics**: Analyze your downloads folder\n\u2705 **Cross-Platform**: Works on Windows, Mac, and Linux\n\u2705 **Safe Operation**: Won't overwrite existing files\n\nThe script will create these folders in your Downloads directory:\n- Images (jpg, png, gif, etc.)\n- Documents (pdf, doc, txt, etc.)\n- Videos (mp4, avi, mkv, etc.)\n- Audio (mp3, wav, flac, etc.)\n- Archives (zip, rar, 7z, etc.)\n- Code (py, js, html, etc.)\n- Others (everything else)\n\nWant me to add any specific file types or modify the categorization?"
            ]
          }
        }
      }
    }
  }
]